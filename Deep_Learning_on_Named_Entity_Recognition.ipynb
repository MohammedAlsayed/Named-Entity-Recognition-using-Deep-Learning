{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning on Named Entity Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Bidirectional LSTM for NER\n",
        "\n",
        "- In this notebook I applied BiLSTM to predict the entity of a word. \n",
        "- To enhance the simple BiLSTM, GloVe word2vec was used as word embeddings\n",
        "\n",
        "**Achieved 98% accuracy using BiLSTM and GloVe embeddings.**"
      ],
      "metadata": {
        "id": "B1nVCeno2ph6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNIh1TzTO1s0",
        "outputId": "b12bd1c2-8cc7-4b62-f79e-563929df0003"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAco9DhzRQP0"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import json\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from tqdm import trange\n",
        "import numpy.ma as ma\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trQOk4db29Tj"
      },
      "source": [
        "## Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsteJ-yhaDsM"
      },
      "source": [
        "PAD_WORD = '<pad>'\n",
        "PAD_TAG = 'O'\n",
        "UNK_WORD = 'UNK'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V12HJ4eSO33O"
      },
      "source": [
        "def update_vocab(txt_path, vocab, is_word):\n",
        "    \"\"\"Update word and tag vocabulary from dataset\n",
        "    Args:\n",
        "        txt_path: (string) path to file, one sentence per line\n",
        "        vocab: (dict or Counter) with update method\n",
        "        is_word: count words, otherwise tags\n",
        "    Returns:\n",
        "        dataset_size: (int) number of elements in the dataset\n",
        "    \"\"\"\n",
        "    sentence_counter = 0\n",
        "    with open(txt_path) as f:\n",
        "        for i, line in enumerate(f):\n",
        "            line_split = line.strip().split(' ')\n",
        "            if(len(line_split) > 1 and is_word):\n",
        "              word = line_split[1].strip()\n",
        "              vocab.update([word])\n",
        "            elif (len(line_split) > 1 and not is_word):\n",
        "              tag = line_split[2].strip()\n",
        "              vocab.update([tag])\n",
        "            elif(len(line_split) == 1):\n",
        "              sentence_counter += 1\n",
        "    return sentence_counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdIZ4M0YThqQ"
      },
      "source": [
        "def save_vocab_to_txt_file(vocab, txt_path):\n",
        "    \"\"\"Writes one token per line, 0-based line id corresponds to the id of the token.\n",
        "    Args:\n",
        "        vocab: (iterable object) yields token\n",
        "        txt_path: (stirng) path to vocab file\n",
        "    \"\"\"\n",
        "    with open(txt_path, \"w\") as f:\n",
        "        for token in vocab:\n",
        "            f.write(token + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLbMMbMPJwf3"
      },
      "source": [
        "base_path = \"/content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/\"\n",
        "train_path = base_path+\"data/train\"\n",
        "dev_path = base_path+ \"data/dev\"\n",
        "test_path = base_path+ \"data/test\"\n",
        "vocab_path = base_path+\"vocab.pickle\"\n",
        "tagmap_path = base_path+\"tagmap.pickle\"\n",
        "\n",
        "words = Counter()\n",
        "size_train_sentences = update_vocab(train_path, words, True)\n",
        "size_dev_sentences = update_vocab(dev_path, words, True)\n",
        "size_test_sentences = update_vocab(test_path, words, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWmrtlhWRNZm"
      },
      "source": [
        "tags = Counter()\n",
        "size_train_tags = update_vocab(train_path, tags, False)\n",
        "size_dev_tags = update_vocab(dev_path, tags, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V10O-0VJR870"
      },
      "source": [
        "words = [tok for tok, count in words.items() if count >= 3]\n",
        "tags = [tok for tok, count in tags.items() if count >= 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FFwa6ooSy23"
      },
      "source": [
        "if PAD_WORD not in words: words.append(PAD_WORD)\n",
        "if PAD_TAG not in tags: tags.append(PAD_TAG)\n",
        "words.append(UNK_WORD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2FGJdkW3Vk"
      },
      "source": [
        "words_path = base_path+\"data/words.txt\"\n",
        "save_vocab_to_txt_file(words, words_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkJb_PXsYJyM"
      },
      "source": [
        "tags_path = base_path+\"data/tags.txt\"\n",
        "save_vocab_to_txt_file(tags, tags_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wcRNM46YgkJ"
      },
      "source": [
        "sizes = {\n",
        "        'train_size': size_train_sentences,   #number of sentences not # of rows!! \n",
        "        'dev_size': size_dev_sentences,       #number of sentences not # of rows!! \n",
        "        'test_size': size_test_sentences,     #number of sentences not # of rows!! \n",
        "        'vocab_size': len(words),\n",
        "        'number_of_tags': len(tags),\n",
        "        'pad_word': PAD_WORD,\n",
        "        'pad_tag': PAD_TAG,\n",
        "        'unk_word': UNK_WORD\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txkHeFd4Yhtu",
        "outputId": "b50222af-420a-4fb6-bfde-8952e3c39b28"
      },
      "source": [
        "print(sizes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_size': 14986, 'dev_size': 3465, 'test_size': 3683, 'vocab_size': 10622, 'number_of_tags': 9, 'pad_word': '<pad>', 'pad_tag': 'O', 'unk_word': 'UNK'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxGg0gVyYjhK"
      },
      "source": [
        "vocab = {}\n",
        "with open(words_path) as f:\n",
        "    for i, l in enumerate(f.read().splitlines()):\n",
        "        vocab[l] = i\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-DjQjDvBexP"
      },
      "source": [
        "with open(vocab_path, 'wb') as handle:\n",
        "    pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88HN_mKqaT-1"
      },
      "source": [
        "tag_map = {}\n",
        "with open(tags_path) as f:\n",
        "    for i, l in enumerate(f.read().splitlines()):\n",
        "        tag_map[l] = i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNYNukFKCpi4"
      },
      "source": [
        "with open(tagmap_path, 'wb') as handle:\n",
        "    pickle.dump(tag_map, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF9tZCuFa5M9"
      },
      "source": [
        "def text2id(txt_path, vocab, tag_map, d, isTest):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "      txt_path: (string) path to the file with sentence and tags\n",
        "      d: (dict) a dictionary in which the loaded data is stored\n",
        "  \"\"\"\n",
        "  sentences = []        \n",
        "  labels = []\n",
        "  s = []\n",
        "  l = []\n",
        "  with open(txt_path) as f:\n",
        "      for token in f.read().splitlines():\n",
        "          #replace each token by its index if it is in vocab\n",
        "          #else use index of UNK\n",
        "          if (len(token.split(' ')) == 1): ## empty line, end of sentence\n",
        "            # append sentence with labels\n",
        "            sentences.append(s)\n",
        "            labels.append(l)\n",
        "            # reset sentence and label list\n",
        "            s = []\n",
        "            l = []\n",
        "            continue\n",
        "          if (len(token.split(' ')) > 1 and isTest):\n",
        "            word = token.split(' ')[1]\n",
        "            if word in vocab:\n",
        "              word = vocab[word]\n",
        "            else:\n",
        "              word = vocab['UNK']\n",
        "            s.append(word)\n",
        "            continue\n",
        "          if (len(token.split(' ')) > 1): # not an empty line\n",
        "            word = token.split(' ')[1]\n",
        "            label = token.split(' ')[2]\n",
        "            # word found in vocab dict\n",
        "            if word in vocab:\n",
        "              word = vocab[word]\n",
        "            else:\n",
        "              word = vocab['UNK']\n",
        "            label = tag_map[label]\n",
        "            s.append(word)\n",
        "            l.append(label)\n",
        "  \n",
        "  if(not isTest):\n",
        "    assert len(labels) == len(sentences)\n",
        "    for i in range(len(labels)):\n",
        "        assert len(labels[i]) == len(sentences[i])\n",
        "    d['data'] = sentences\n",
        "    d['labels'] = labels\n",
        "    d['size'] = len(sentences)\n",
        "  if(isTest):\n",
        "    d['data'] = sentences\n",
        "    d['size'] = len(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfsG4vgefMB5"
      },
      "source": [
        "train_data = {}\n",
        "text2id(train_path, vocab, tag_map, train_data, False)\n",
        "dev_data = {}\n",
        "text2id(dev_path, vocab, tag_map, dev_data, False)\n",
        "test_data = {}\n",
        "text2id(test_path, vocab, tag_map, test_data, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXeHu-Ez3DRE"
      },
      "source": [
        "## Pytorch Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMegG8tt0BD3"
      },
      "source": [
        "def data_iterator(data, params, vocab, shuffle=False):\n",
        "        \"\"\"\n",
        "        Returns a generator that yields batches data with labels. Batch size is params.batch_size. Expires after one\n",
        "        pass over the data.\n",
        "        Args:\n",
        "            data: (dict) contains data which has keys 'data', 'labels' and 'size'\n",
        "            params: (Params) hyperparameters of the training process.\n",
        "            shuffle: (bool) whether the data should be shuffled\n",
        "        Yields:\n",
        "            batch_data: (Variable) dimension batch_size x seq_len with the sentence data\n",
        "            batch_labels: (Variable) dimension batch_size x seq_len with the corresponding labels\n",
        "        \"\"\"\n",
        "\n",
        "        # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n",
        "        order = list(range(data['size']))\n",
        "        if shuffle:\n",
        "            random.seed(230)\n",
        "            random.shuffle(order)\n",
        "\n",
        "        # one pass over data\n",
        "        for i in range((data['size']+1)//params['batch_size']):\n",
        "            # fetch sentences and tags\n",
        "            batch_sentences = [data['data'][idx] for idx in order[i*params['batch_size']:(i+1)*params['batch_size']]]\n",
        "            batch_tags = [data['labels'][idx] for idx in order[i*params['batch_size']:(i+1)*params['batch_size']]]\n",
        "\n",
        "            # compute length of longest sentence in batch\n",
        "            batch_max_len = max([len(s) for s in batch_sentences])\n",
        "\n",
        "            # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
        "            # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
        "            batch_data = vocab[PAD_WORD]*np.ones((len(batch_sentences), batch_max_len))\n",
        "            batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
        "\n",
        "            # copy the data to the numpy array\n",
        "            for j in range(len(batch_sentences)):\n",
        "                cur_len = len(batch_sentences[j])\n",
        "                batch_data[j][:cur_len] = batch_sentences[j]\n",
        "                batch_labels[j][:cur_len] = batch_tags[j]\n",
        "\n",
        "            # since all data are indices, we convert them to torch LongTensors\n",
        "            batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
        "\n",
        "            # shift tensors to GPU if available\n",
        "            if params['cuda']:\n",
        "                batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
        "\n",
        "            # convert them to Variables to record operations in the computational graph\n",
        "            batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
        "    \n",
        "            yield batch_data, batch_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8AFUkaJ8FxV"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    This is the standard way to define your own network in PyTorch. You typically choose the components\n",
        "    (e.g. LSTMs, linear layers etc.) of your network in the __init__ function. You then apply these layers\n",
        "    on the input step-by-step in the forward function. You can use torch.nn.functional to apply functions\n",
        "    such as F.relu, F.sigmoid, F.softmax. Be careful to ensure your dimensions are correct after each step.\n",
        "    You are encouraged to have a look at the network in pytorch/vision/model/net.py to get a better sense of how\n",
        "    you can go about defining your own network.\n",
        "    The documentation for all the various components available to you is here: http://pytorch.org/docs/master/nn.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params):\n",
        "        \"\"\"\n",
        "        We define an recurrent network that predicts the NER tags for each token in the sentence. The components\n",
        "        required are:\n",
        "        - an embedding layer: this layer maps each index in range(params.vocab_size) to a params.embedding_dim vector\n",
        "        - lstm: applying the LSTM on the sequential input returns an output for each token in the sentence\n",
        "        - fc: a fully connected layer that converts the LSTM output for each token to a distribution over NER tags\n",
        "        Args:\n",
        "            params: (Params) contains vocab_size, embedding_dim, lstm_hidden_dim\n",
        "        \"\"\"\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # the embedding takes as input the vocab_size and the embedding_dim\n",
        "        self.embedding = nn.Embedding(params['vocab_size'], params['embedding_dim'])\n",
        "        # self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(params['embeddings']).float(), freeze=False)\n",
        "        \n",
        "        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n",
        "        # for more details on how to use it, check out the documentation\n",
        "        self.lstm = nn.LSTM(params['embedding_dim'],\n",
        "                            params['lstm_hidden_dim'],\n",
        "                            num_layers = 1,\n",
        "                            bidirectional=True,\n",
        "                            dropout=0.33,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.linear = nn.Linear(2*params['lstm_hidden_dim'], params['linear_output_dim'])\n",
        "        # the fully connected layer transforms the output to give the final output layer\n",
        "        self.fc = nn.Linear(params['linear_output_dim'], params['number_of_tags'])\n",
        "\n",
        "    def forward(self, s):\n",
        "        \"\"\"\n",
        "        This function defines how we use the components of our network to operate on an input batch.\n",
        "        Args:\n",
        "            s: (Variable) contains a batch of sentences, of dimension batch_size x seq_len, where seq_len is\n",
        "               the length of the longest sentence in the batch. For sentences shorter than seq_len, the remaining\n",
        "               tokens are PADding tokens. Each row is a sentence with each element corresponding to the index of\n",
        "               the token in the vocab.\n",
        "        Returns:\n",
        "            out: (Variable) dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
        "                 of each sentence.\n",
        "        Note: the dimensions after each step are provided\n",
        "        \"\"\"\n",
        "        #                                -> batch_size x seq_len\n",
        "        # apply the embedding layer that maps each token to its embedding\n",
        "        # dim: batch_size x seq_len x embedding_dim\n",
        "        s = self.embedding(s)\n",
        "\n",
        "        # run the LSTM along the sentences of length seq_len\n",
        "        # dim: batch_size x seq_len x lstm_hidden_dim\n",
        "        s, _ = self.lstm(s)\n",
        "\n",
        "        # make the Variable contiguous in memory (a PyTorch artefact)\n",
        "        s = s.contiguous()\n",
        "\n",
        "        s = self.linear(s)\n",
        "\n",
        "        m = nn.ELU()\n",
        "        s = m(s)\n",
        "\n",
        "        # reshape the Variable so that each row contains one token\n",
        "        # dim: batch_size*seq_len x lstm_hidden_dim\n",
        "        s = s.view(-1, s.shape[2])\n",
        "        \n",
        "\n",
        "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
        "        s = self.fc(s)                   # dim: batch_size*seq_len x num_tags\n",
        "\n",
        "        # apply log softmax on each token's output (this is recommended over applying softmax\n",
        "        # since it is numerically more stable)\n",
        "        return F.log_softmax(s, dim=1)   # dim: batch_size*seq_len x num_tags\n",
        "\n",
        "\n",
        "def loss_fn(outputs, labels):\n",
        "    \"\"\"\n",
        "    Compute the cross entropy loss given outputs from the model and labels for all tokens. Exclude loss terms\n",
        "    for PADding tokens.\n",
        "    Args:\n",
        "        outputs: (Variable) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
        "        labels: (Variable) dimension batch_size x seq_len where each element is either a label in [0, 1, ... num_tag-1],\n",
        "                or -1 in case it is a PADding token.\n",
        "    Returns:\n",
        "        loss: (Variable) cross entropy loss for all tokens in the batch\n",
        "    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n",
        "          demonstrates how you can easily define a custom loss function.\n",
        "    \"\"\"\n",
        "\n",
        "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
        "    labels = labels.view(-1)\n",
        "\n",
        "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
        "    mask = (labels >= 0).float()\n",
        "\n",
        "    # indexing with negative values is not supported. Since PADded tokens have label -1, we convert them to a positive\n",
        "    # number. This does not affect training, since we ignore the PADded tokens with the mask.\n",
        "    labels = labels % outputs.shape[1]\n",
        "\n",
        "    num_tokens = int(torch.sum(mask))\n",
        "\n",
        "    # compute cross entropy loss for all tokens (except PADding tokens), by multiplying with mask.\n",
        "    return -torch.sum(outputs[range(outputs.shape[0]), labels]*mask)/num_tokens\n",
        "\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    \"\"\"\n",
        "    Compute the accuracy, given the outputs and labels for all tokens. Exclude PADding terms.\n",
        "    Args:\n",
        "        outputs: (np.ndarray) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
        "        labels: (np.ndarray) dimension batch_size x seq_len where each element is either a label in\n",
        "                [0, 1, ... num_tag-1], or -1 in case it is a PADding token.\n",
        "    Returns: (float) accuracy in [0,1]\n",
        "    \"\"\"\n",
        "\n",
        "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
        "    labels = labels.ravel()\n",
        "\n",
        "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
        "    mask = (labels >= 0)\n",
        "\n",
        "    # np.argmax gives us the class predicted for each token by the model\n",
        "    outputs = np.argmax(outputs, axis=1)\n",
        "\n",
        "    # compare outputs with labels and divide by number of tokens (excluding PADding tokens)\n",
        "    return np.sum(outputs == labels)/float(np.sum(mask))\n",
        "\n",
        "# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\n",
        "metrics = {\n",
        "    'accuracy': accuracy,\n",
        "    # could add more metrics such as accuracy for each token type\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EilSUHIVCSLV"
      },
      "source": [
        "class RunningAverage():\n",
        "    \"\"\"A simple class that maintains the running average of a quantity\n",
        "    Example:\n",
        "    ```\n",
        "    loss_avg = RunningAverage()\n",
        "    loss_avg.update(2)\n",
        "    loss_avg.update(4)\n",
        "    loss_avg() = 3\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.steps = 0\n",
        "        self.total = 0\n",
        "\n",
        "    def update(self, val):\n",
        "        self.total += val\n",
        "        self.steps += 1\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.total / float(self.steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjGF4CYIKgRE"
      },
      "source": [
        "def evaluate(model, loss_fn, data_iterator, metrics, params, num_steps):\n",
        "    \"\"\"Evaluate the model on `num_steps` batches.\n",
        "    Args:\n",
        "        model: (torch.nn.Module) the neural network\n",
        "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
        "        data_iterator: (generator) a generator that generates batches of data and labels\n",
        "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
        "        params: (Params) hyperparameters\n",
        "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
        "    \"\"\"\n",
        "\n",
        "    # set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # summary for current eval loop\n",
        "    summ = []\n",
        "\n",
        "    # compute metrics over the dataset\n",
        "    for _ in range(num_steps):\n",
        "        # fetch the next evaluation batch\n",
        "        data_batch, labels_batch = next(data_iterator)\n",
        "        \n",
        "        # compute model output\n",
        "        output_batch = model(data_batch)\n",
        "        loss = loss_fn(output_batch, labels_batch)\n",
        "\n",
        "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
        "        output_batch = output_batch.data.cpu().numpy()\n",
        "        labels_batch = labels_batch.data.cpu().numpy()\n",
        "\n",
        "        # compute all metrics on this batch\n",
        "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
        "                         for metric in metrics}\n",
        "        summary_batch['loss'] = loss.item()\n",
        "        summ.append(summary_batch)\n",
        "\n",
        "    # compute mean of all metrics in summary\n",
        "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
        "    print(\"- Eval metrics : \" + metrics_string)\n",
        "    return metrics_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CwG0sRjRRyI"
      },
      "source": [
        "def train(model, optimizer, scheduler, loss_fn, data_iterator, metrics, params, num_steps):\n",
        "    \"\"\"Train the model on `num_steps` batches\n",
        "    Args:\n",
        "        model: (torch.nn.Module) the neural network\n",
        "        optimizer: (torch.optim) optimizer for parameters of model\n",
        "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
        "        data_iterator: (generator) a generator that generates batches of data and labels\n",
        "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
        "        params: (Params) hyperparameters\n",
        "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
        "    \"\"\"\n",
        "\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # summary for current training loop and a running average object for loss\n",
        "    summ = []\n",
        "    loss_avg = RunningAverage()\n",
        "\n",
        "    # Use tqdm for progress bar\n",
        "    t = trange(num_steps)\n",
        "    for i in t:\n",
        "        # fetch the next training batch\n",
        "        train_batch, labels_batch = next(data_iterator)\n",
        "\n",
        "        # compute model output and loss\n",
        "        output_batch = model(train_batch)\n",
        "        loss = loss_fn(output_batch, labels_batch)\n",
        "\n",
        "        # clear previous gradients, compute gradients of all variables wrt loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # performs updates using calculated gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # Evaluate summaries only once in a while\n",
        "        if i % params['save_summary_steps'] == 0:\n",
        "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
        "            output_batch = output_batch.data.cpu().numpy()\n",
        "            labels_batch = labels_batch.data.cpu().numpy()\n",
        "\n",
        "            # compute all metrics on this batch\n",
        "            summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
        "                             for metric in metrics}\n",
        "            summary_batch['loss'] = loss.item()\n",
        "            summ.append(summary_batch)\n",
        "        \n",
        "        # update the average loss\n",
        "        loss_avg.update(loss.item())\n",
        "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
        "\n",
        "    # update the learning rate\n",
        "    scheduler.step()\n",
        "    # compute mean of all metrics in summary\n",
        "    metrics_mean = {metric: np.mean([x[metric]\n",
        "                                     for x in summ]) for metric in summ[0]}\n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v)\n",
        "                                for k, v in metrics_mean.items())\n",
        "    print(\"- Train metrics: \" + metrics_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVXW9GySB1_j"
      },
      "source": [
        "def train_and_evaluate(model, train_data, val_data, vocab, optimizer, scheduler, loss_fn, metrics, params):\n",
        "    \"\"\"Train the model and evaluate every epoch.\n",
        "    Args:\n",
        "        model: (torch.nn.Module) the neural network\n",
        "        train_data: (dict) training data with keys 'data' and 'labels'\n",
        "        val_data: (dict) validaion data with keys 'data' and 'labels'\n",
        "        optimizer: (torch.optim) optimizer for parameters of model\n",
        "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
        "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
        "        params: (Params) hyperparameters\n",
        "        model_dir: (string) directory containing config, weights and log\n",
        "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
        "    \"\"\"\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(params['num_epochs']):\n",
        "        # Run one epoch\n",
        "        print(\"Epoch {}/{}\".format(epoch + 1, params['num_epochs']))\n",
        "\n",
        "        # compute number of batches in one epoch (one full pass over the training set)\n",
        "        num_steps = (params['train_size'] + 1) // params['batch_size']\n",
        "        train_data_iterator = data_iterator(train_data, params, vocab, shuffle=True)\n",
        "        train(model, optimizer, scheduler, loss_fn, train_data_iterator,\n",
        "              metrics, params, num_steps)\n",
        "\n",
        "        # Evaluate for one epoch on validation set\n",
        "        num_steps = (params['val_size'] + 1) // params['batch_size']\n",
        "        val_data_iterator = data_iterator(\n",
        "            val_data, params, vocab, shuffle=False)\n",
        "        val_metrics = evaluate(\n",
        "            model, loss_fn, val_data_iterator, metrics, params, num_steps)\n",
        "        val_acc = val_metrics['accuracy']\n",
        "        is_best = val_acc >= best_val_acc\n",
        "\n",
        "        # If best_eval, best_save_path\n",
        "        if is_best:\n",
        "            print(\"- Found new best accuracy\")\n",
        "            best_val_acc = val_acc\n",
        "            print(best_val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P573uuOU3NGp"
      },
      "source": [
        "#Task 1: Simple Bidirectional LSTM model (40 points)\n",
        "The below hyperparameters gave the following result on the dev file:\n",
        "```\n",
        "{\n",
        "    \"learning_rate\": [0.2],\n",
        "    \"momentum\":[0.9],\n",
        "    \"batch_size\": [5],\n",
        "    \"num_epochs\":[10],\n",
        "    \"lstm_hidden_dim\": 256,\n",
        "    \"linear_output_dim\": 128,\n",
        "    \"embedding_dim\": 100,\n",
        "    \"cuda\":true,\n",
        "    \"save_summary_steps\": 100\n",
        "    \"weight_decay\":0.0002\n",
        "    \"scheduler_StepLR_size\":5,\n",
        "    \"scheduler_StepLR_gamma\":0.5,\n",
        "    \"vocab_threshold\": >= 3\n",
        "}\n",
        "```\n",
        "Perl *result*\n",
        "```\n",
        "processed 51577 tokens with 5942 phrases; found: 5305 phrases; correct: 4505.\n",
        "accuracy:  96.00%; precision:  84.92%; recall:  75.82%; FB1:  80.11\n",
        "              LOC: precision:  87.85%; recall:  85.03%; FB1:  86.42  1778\n",
        "             MISC: precision:  84.75%; recall:  75.92%; FB1:  80.09  826\n",
        "              ORG: precision:  75.65%; recall:  69.05%; FB1:  72.20  1224\n",
        "              PER: precision:  89.17%; recall:  71.50%; FB1:  79.36  1477\n",
        "```\n",
        "The model Archeticture is:\n",
        "```\n",
        "Net(\n",
        "  (embedding): Embedding(30292, 100)\n",
        "  (lstm): LSTM(100, 256, batch_first=True, dropout=0.33, bidirectional=True)\n",
        "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
        "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoQus9RBl7Nf"
      },
      "source": [
        "models_dict = {} #Save the best model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcpD_xNLllbT",
        "outputId": "3625bb38-2f5a-4be8-dbb7-5ec067ee90f6"
      },
      "source": [
        "ix_to_tag = {v: k for k, v in tag_map.items()}\n",
        "json_path = \"/content/params.json\"\n",
        "with open(json_path) as f:\n",
        "      params = json.load(f)\n",
        "params.update({'vocab_size':sizes['vocab_size']})\n",
        "params.update({'number_of_tags':sizes['number_of_tags']})\n",
        "params.update({'val_size':sizes['dev_size']})\n",
        "params.update({'train_size':sizes['train_size']})\n",
        "params.update({'test_size':sizes['test_size']})\n",
        "\n",
        "model_params = {} \n",
        "for b in params['batch_size']:\n",
        "  for m in params['momentum']:\n",
        "    for l in params['learning_rate']:\n",
        "      for e in params['num_epochs']:\n",
        "        model_params['learning_rate'] = l\n",
        "        model_params['momentum'] = m\n",
        "        model_params['batch_size'] = b\n",
        "        model_params['num_epochs'] = e\n",
        "        model_params['lstm_hidden_dim'] = params['lstm_hidden_dim']\n",
        "        model_params['linear_output_dim'] = params['linear_output_dim']\n",
        "        model_params['embedding_dim'] = params['embedding_dim']\n",
        "        model_params['cuda'] = params['cuda']\n",
        "        model_params['save_summary_steps'] = params['save_summary_steps']\n",
        "        model_params['vocab_size'] = params['vocab_size']\n",
        "        model_params['number_of_tags'] = params['number_of_tags']\n",
        "        model_params['val_size'] = params['val_size']\n",
        "        model_params['train_size'] = params['train_size']\n",
        "        \n",
        "        model_name = \"lr{}_momnt{}_batch{}_epo{}\".format(l,m,b,e)\n",
        "        print(\"## model name: {}\".format(model_name))\n",
        "        \n",
        "        model = Net(model_params)\n",
        "        if params['cuda']:\n",
        "          model = Net(model_params).cuda()\n",
        "\n",
        "        optimizer = optim.SGD(model.parameters(), weight_decay=0.0002, momentum=model_params['momentum'], lr=model_params['learning_rate'])\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "        train_and_evaluate(model, train_data, dev_data, vocab, optimizer,scheduler ,loss_fn, metrics, model_params)\n",
        "        \n",
        "\n",
        "        models_dict[model_name] = {'model':model, 'batch_size': b, 'cuda': True}\n",
        "\n",
        "        num_steps = (params['val_size'] + 1) // model_params['batch_size']\n",
        "        val_data_iterator = data_iterator(\n",
        "                  dev_data, model_params, vocab, shuffle=False)\n",
        "        model.eval()\n",
        "        print(\"writing predicted tokens to ner_{}\".format(model_name))\n",
        "        with open(base_path+\"/12/ner/ner_{}\".format(model_name), \"w\") as f:\n",
        "          for _ in range(num_steps):\n",
        "              # fetch the next evaluation batch\n",
        "              data_batch, labels_batch = next(val_data_iterator)\n",
        "              \n",
        "              # compute model output\n",
        "              output_batch = model(data_batch)\n",
        "              output_batch = output_batch.data.cpu().numpy()\n",
        "              labels_batch = labels_batch.data.cpu().numpy()        \n",
        "              labels = labels_batch.ravel()\n",
        "              mask = (labels < 0)\n",
        "\n",
        "              outputs = np.argmax(output_batch, axis=1)\n",
        "              outputs_nopadding = ma.array(outputs, mask=mask)\n",
        "              for ner in outputs_nopadding.compressed():\n",
        "                  y = ix_to_tag[ner]\n",
        "                  f.write(y + '\\n')\n",
        "\n",
        "        with open(dev_path, \"r\") as f:\n",
        "          dev_lines = f.read().splitlines()\n",
        "\n",
        "        with open(base_path+\"/12/ner/ner_{}\".format(model_name), \"r\") as f:\n",
        "          pred_lines = f.read().splitlines()\n",
        "\n",
        "        print(\"writing predicted NER to dev_{}\".format(model_name))\n",
        "        pred_counter = 0\n",
        "        with open(base_path+\"/12/pred/dev_{}\".format(model_name), \"w\") as f: \n",
        "          for line in dev_lines:\n",
        "            if (line == \"\"):\n",
        "              f.write(\"\\n\")\n",
        "            else:\n",
        "              if pred_counter < len(pred_lines): # check why last dev sentence is out of index\n",
        "                f.write(\"{} {}\\n\".format(line, pred_lines[pred_counter]))\n",
        "                pred_counter += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## model name: lr0.2_momnt0.9_batch5_epo10\n",
            "writing predicted tokens to ner_lr0.2_momnt0.9_batch5_epo10\n",
            "writing predicted NER to dev_lr0.2_momnt0.9_batch5_epo10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QOA059A56Nz"
      },
      "source": [
        "# Save model for task1\n",
        "model_path = base_path+\"blstm1.pt\"\n",
        "torch.save(model.state_dict(), model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXjQuIqQGFiu"
      },
      "source": [
        "def test_iterator(data, params, vocab):\n",
        "\n",
        "        order = list(range(data['size']))\n",
        "        # one pass over data\n",
        "        for i in range((data['size']+1)//params['batch_size']):\n",
        "            # fetch sentences and tags\n",
        "            batch_sentences = [data['data'][idx] for idx in order[i*params['batch_size']:(i+1)*params['batch_size']]]\n",
        "\n",
        "\n",
        "            # compute length of longest sentence in batch\n",
        "            batch_max_len = max([len(s) for s in batch_sentences])\n",
        "\n",
        "            # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n",
        "            # initialising labels to -1 differentiates tokens with tags from PADding tokens\n",
        "            batch_data = vocab[PAD_WORD]*np.ones((len(batch_sentences), batch_max_len))\n",
        "            batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
        "            # copy the data to the numpy array\n",
        "            for j in range(len(batch_sentences)):\n",
        "                cur_len = len(batch_sentences[j])\n",
        "                batch_data[j][:cur_len] = batch_sentences[j]\n",
        "                batch_labels[j][:cur_len] = 1\n",
        "\n",
        "            # since all data are indices, we convert them to torch LongTensors\n",
        "            batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
        "\n",
        "            # shift tensors to GPU if available\n",
        "            if params['cuda']:\n",
        "                batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
        "\n",
        "            # convert them to Variables to record operations in the computational graph\n",
        "            batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
        "    \n",
        "            yield batch_data, batch_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4WDiU8a84dF"
      },
      "source": [
        "def predict(model, test_path, test_data, data_iterator, num_steps, output_path):\n",
        "  with open(test_path, \"r\") as f:\n",
        "    test_lines = f.read().splitlines()\n",
        "\n",
        "  model.eval()\n",
        "  predicted_ner_path = base_path + \"pred_ner\"\n",
        "  print(\"predicting NER and saving it to {}\".format(predicted_ner_path))\n",
        "  with open(predicted_ner_path, \"w\") as f:\n",
        "    for i in range(num_steps):\n",
        "        # fetch the next evaluation batch\n",
        "        batch_data, batch_labels = next(data_iterator)\n",
        "        # compute model output\n",
        "\n",
        "        output_batch = model(batch_data)\n",
        "        output_batch = output_batch.data.cpu().numpy()\n",
        "        output_labels = batch_labels.data.cpu().numpy()        \n",
        "        labels = output_labels.ravel()\n",
        "        mask = (labels < 0)\n",
        "\n",
        "        outputs = np.argmax(output_batch, axis=1)\n",
        "        outputs_nopadding = ma.array(outputs, mask=mask)\n",
        "\n",
        "        for ner in outputs_nopadding.compressed():\n",
        "            y = ix_to_tag[ner]\n",
        "            f.write(y + '\\n')\n",
        "        \n",
        "  print(\"reading {}\".format(test_path))\n",
        "  with open(test_path, \"r\") as f:\n",
        "    test_lines = f.read().splitlines()\n",
        "\n",
        "  print(\"reading {}\".format(predicted_ner_path))\n",
        "  with open(predicted_ner_path, \"r\") as f:\n",
        "    pred_lines = f.read().splitlines()\n",
        "\n",
        "  print(\"appending predicted NER to {}\".format(output_path))\n",
        "  pred_counter = 0\n",
        "  new_line_counter = 0\n",
        "  with open(base_path+output_path, \"w\") as f: \n",
        "    for idx, line in enumerate(test_lines):\n",
        "      if (line == \"\"):\n",
        "        f.write(\"\\n\")\n",
        "        new_line_counter += 1\n",
        "      else:\n",
        "        if pred_counter < len(pred_lines): # check why last dev sentence is out of index\n",
        "          f.write(\"{} {}\\n\".format(line, pred_lines[pred_counter]))\n",
        "          pred_counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMDG6inrl4--"
      },
      "source": [
        "test_data_iterator = test_iterator(test_data, model_params, vocab)\n",
        "num_steps = (test_data['size'] + 1) // params['batch_size'][0]\n",
        "predict(model, test_path, test_data, test_data_iterator, num_steps, \"test1.out\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1x7FlYXaP3p",
        "outputId": "af4e9478-75a2-4e37-8b46-3aa4f157509a"
      },
      "source": [
        "val_data_iterator = data_iterator(dev_data, model_params, vocab, shuffle=False)\n",
        "num_steps = (dev_data['size'] + 1) // params['batch_size'][0]\n",
        "predict(model, dev_path, dev_data, val_data_iterator, num_steps, \"dev.out\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicting NER and saving it to /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/pred_ner\n",
            "reading /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/data/dev\n",
            "reading /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/pred_ner\n",
            "appending predicted NER to dev_test.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLjxSG9mlXSW"
      },
      "source": [
        "## Load the model and predict test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdSkQTtOM2ph",
        "outputId": "94e65422-92ae-4e2c-9314-335a67e2c7ca"
      },
      "source": [
        "load_model_params = {\n",
        " 'batch_size': 5,\n",
        " 'cuda': True,\n",
        " 'embedding_dim': 100,\n",
        " 'linear_output_dim': 128,\n",
        " 'lstm_hidden_dim': 256,\n",
        " 'number_of_tags': 9,\n",
        " 'save_summary_steps': 100,\n",
        " 'test_size': 3683,\n",
        " 'vocab_size': 9412}\n",
        "\n",
        "\n",
        "test_data_iterator = test_iterator(test_data, load_model_params, vocab)\n",
        "load_model = Net(load_model_params).cuda()\n",
        "load_model.load_state_dict(torch.load(model_path))\n",
        "num_steps = (load_model_params['test_size'] + 1) // load_model_params['batch_size']\n",
        "predict(model, test_path, test_data, test_data_iterator, num_steps, \"test1.out\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.33 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicting NER and saving it to /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/pred_ner\n",
            "reading /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/data/test\n",
            "reading /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/pred_ner\n",
            "appending predicted NER to test1.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFkNRwbroSv7"
      },
      "source": [
        "# Task 2:  Using GloVe word embeddings\n",
        "```\n",
        "{\n",
        "    \"learning_rate\": 0.2,\n",
        "    \"momentum\":0.9,\n",
        "    \"batch_size\": 5,\n",
        "    \"num_epochs\":5,\n",
        "    \"lstm_hidden_dim\": 256,\n",
        "    \"linear_output_dim\": 128,\n",
        "    \"embedding_dim\": 100,\n",
        "    \"cuda\":true,\n",
        "    \"weight_decay\":0.00001,\n",
        "    \"scheduler_StepLR_size\":3,\n",
        "    \"scheduler_StepLR_gamma\":0.5\n",
        "    \"vocabular threshold: all vocab\n",
        "}\n",
        "```\n",
        "\n",
        "Perl *result*\n",
        "```\n",
        "processed 51577 tokens with 5942 phrases; found: 5997 phrases; correct: 5385.\n",
        "accuracy:  98.17%; precision:  89.79%; recall:  90.63%; FB1:  90.21\n",
        "              LOC: precision:  93.78%; recall:  94.45%; FB1:  94.11  1850\n",
        "             MISC: precision:  82.94%; recall:  83.84%; FB1:  83.39  932\n",
        "              ORG: precision:  84.60%; recall:  84.41%; FB1:  84.51  1338\n",
        "              PER: precision:  92.97%; recall:  94.73%; FB1:  93.84  1877\n",
        "```\n",
        "\n",
        "The neural net archeticture:\n",
        "\n",
        "- 30292 is the vocab size\n",
        "```\n",
        "Net(\n",
        "  (embedding): Embedding(30292, 100)\n",
        "  (lstm): LSTM(100, 256, batch_first=True, dropout=0.33, bidirectional=True)\n",
        "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
        "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD1DPkMYuLJp"
      },
      "source": [
        "words = Counter()\n",
        "size_train_sentences = update_vocab(train_path, words, True)\n",
        "size_dev_sentences = update_vocab(dev_path, words, True)\n",
        "size_test_sentences = update_vocab(test_path, words, True)\n",
        "tags = Counter()\n",
        "size_train_tags = update_vocab(train_path, tags, False)\n",
        "size_dev_tags = update_vocab(dev_path, tags, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAXgb3h9e2gG"
      },
      "source": [
        "words = [tok for tok, count in words.items() if count >= 0]\n",
        "if PAD_WORD not in words: words.append(PAD_WORD)\n",
        "if PAD_TAG not in tags: tags.append(PAD_TAG)\n",
        "words.append(UNK_WORD)\n",
        "words_path = base_path+\"data/words.txt\"\n",
        "save_vocab_to_txt_file(words, words_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crGnMQb1faTb"
      },
      "source": [
        "sizes = {\n",
        "        'train_size': size_train_sentences,   #number of sentences not # of rows!! \n",
        "        'dev_size': size_dev_sentences,       #number of sentences not # of rows!! \n",
        "        'test_size': size_test_sentences,     #number of sentences not # of rows!! \n",
        "        'vocab_size': len(words),\n",
        "        'number_of_tags': len(tags),\n",
        "        'pad_word': PAD_WORD,\n",
        "        'pad_tag': PAD_TAG,\n",
        "        'unk_word': UNK_WORD\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFycBJ1je-f0",
        "outputId": "14854af8-39e6-48a1-c8bb-e7e6c1fea0c5"
      },
      "source": [
        "sizes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_size': 3465,\n",
              " 'number_of_tags': 9,\n",
              " 'pad_tag': 'O',\n",
              " 'pad_word': '<pad>',\n",
              " 'test_size': 3683,\n",
              " 'train_size': 14986,\n",
              " 'unk_word': 'UNK',\n",
              " 'vocab_size': 30292}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBoBWDG3fjvK"
      },
      "source": [
        "vocab = {}\n",
        "with open(words_path) as f:\n",
        "    for i, l in enumerate(f.read().splitlines()):\n",
        "        vocab[l] = i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5NdSlm_f82G"
      },
      "source": [
        "train_data = {}\n",
        "text2id(train_path, vocab, tag_map, train_data, False)\n",
        "dev_data = {}\n",
        "text2id(dev_path, vocab, tag_map, dev_data, False)\n",
        "test_data = {}\n",
        "text2id(test_path, vocab, tag_map, test_data, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb1-UPd6oSaR"
      },
      "source": [
        "import gzip\n",
        "def load_glove_model(glove_path):\n",
        "    print(\"Loading Glove Model\")\n",
        "    glove_model = {}\n",
        "    with gzip.open(glove_path,'rb') as f:\n",
        "        for line in f:\n",
        "            split_line = line.split()\n",
        "            word = split_line[0].decode()\n",
        "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
        "            glove_model[word] = embedding\n",
        "    print(f\"{len(glove_model)} words loaded!\")\n",
        "    return glove_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTIwAJEihpxJ",
        "outputId": "6a8c2c15-dccf-4108-921a-da0d7bb3c1c2"
      },
      "source": [
        "glove_path = base_path+\"glove.6B.100d.gz\"\n",
        "glove = load_glove_model(glove_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Glove Model\n",
            "400000 words loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGImh89alWo9"
      },
      "source": [
        "def create_GloVE_vocab1(words, glove, vocab, glove_vocab):\n",
        "  np.random.seed(42)\n",
        "  pad_word = np.random.rand(100)\n",
        "  for k in words:\n",
        "    idx = k\n",
        "    if k in glove:\n",
        "      glove_vocab[idx] = glove[k]\n",
        "    elif k.lower() in glove:\n",
        "      glove_vocab[idx] = glove[k.lower()]\n",
        "    else:\n",
        "      glove_vocab[idx] = pad_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_2p1BeXpyf7"
      },
      "source": [
        "train_data = {}\n",
        "text2id(train_path, vocab, tag_map, train_data, False)\n",
        "dev_data = {}\n",
        "text2id(dev_path, vocab, tag_map, dev_data, False)\n",
        "test_data = {}\n",
        "text2id(test_path, vocab, tag_map, test_data, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29p7AKJLpED5"
      },
      "source": [
        "embedding_matrix = np.zeros((len(vocab), params['embedding_dim']))\n",
        "np.random.seed(42)\n",
        "for word in words:\n",
        "    index = vocab[word]\n",
        "    if word in glove_vocab:\n",
        "        vector = glove_vocab[word]\n",
        "    elif word.lower() in glove_vocab:\n",
        "        vector = glove_vocab[word.lower()]\n",
        "    else:\n",
        "        vector = np.random.rand(params['embedding_dim'])\n",
        "    embedding_matrix[index] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69_VOOrU5DRK",
        "outputId": "8d41bbfa-404a-4409-ebe7-fa1e517fb208"
      },
      "source": [
        "ix_to_tag = {v: k for k, v in tag_map.items()}\n",
        "json_path = \"/content/params.json\"\n",
        "with open(json_path) as f:\n",
        "      params = json.load(f)\n",
        "params.update({'vocab_size':sizes['vocab_size']})\n",
        "params.update({'number_of_tags':sizes['number_of_tags']})\n",
        "params.update({'val_size':sizes['dev_size']})\n",
        "params.update({'train_size':sizes['train_size']})\n",
        "params.update({'test_size':sizes['test_size']})\n",
        "params.update({'embeddings': embedding_matrix})\n",
        "\n",
        "model = Net(params)\n",
        "print(model)\n",
        "if params['cuda']:\n",
        "  model = Net(params).cuda()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), weight_decay=0.00001, momentum=params['momentum'], lr=params['learning_rate'])\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "train_and_evaluate(model, train_data, dev_data, vocab, optimizer,scheduler ,loss_fn, metrics, params)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.33 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (embedding): Embedding(30292, 100)\n",
            "  (lstm): LSTM(100, 256, batch_first=True, dropout=0.33, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
            ")\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2997/2997 [00:24<00:00, 123.04it/s, loss=0.181]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train metrics: accuracy: 0.915 ; loss: 0.246\n",
            "- Eval metrics : accuracy: 0.970 ; loss: 0.119\n",
            "- Found new best accuracy\n",
            "0.9696443651181217\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2997/2997 [00:24<00:00, 123.65it/s, loss=0.064]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train metrics: accuracy: 0.978 ; loss: 0.075\n",
            "- Eval metrics : accuracy: 0.978 ; loss: 0.100\n",
            "- Found new best accuracy\n",
            "0.9781822297752824\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2997/2997 [00:24<00:00, 123.73it/s, loss=0.031]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train metrics: accuracy: 0.990 ; loss: 0.039\n",
            "- Eval metrics : accuracy: 0.979 ; loss: 0.104\n",
            "- Found new best accuracy\n",
            "0.9787926612258975\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2997/2997 [00:24<00:00, 123.47it/s, loss=0.014]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train metrics: accuracy: 0.993 ; loss: 0.021\n",
            "- Eval metrics : accuracy: 0.982 ; loss: 0.095\n",
            "- Found new best accuracy\n",
            "0.9824336612704332\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2997/2997 [00:24<00:00, 123.77it/s, loss=0.008]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Train metrics: accuracy: 0.997 ; loss: 0.008\n",
            "- Eval metrics : accuracy: 0.983 ; loss: 0.107\n",
            "- Found new best accuracy\n",
            "0.9827497251504924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JvvGPK-q6u2",
        "outputId": "8a716f06-a496-47f7-91a6-f8b406051364"
      },
      "source": [
        "val_data_iterator = data_iterator(dev_data, params, vocab, shuffle=False)\n",
        "num_steps = (dev_data['size'] + 1) // params['batch_size']\n",
        "predict(model, dev_path, dev_data, val_data_iterator, num_steps, \"dev2.out\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicting NER and saving it to /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/pred_ner\n",
            "reading /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/data/dev\n",
            "reading /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/pred_ner\n",
            "appending predicted NER to dev2.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbg7LqNx6BjI"
      },
      "source": [
        "# Save model for task2\n",
        "model2_path = base_path+\"blstm2.pt\"\n",
        "torch.save(model.state_dict(), model2_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-KSxqC72NUS",
        "outputId": "fdd2e930-9818-44b1-df7e-0ac6f0b6a76a"
      },
      "source": [
        "test_data_iterator = test_iterator(test_data, params, vocab)\n",
        "num_steps = (test_data['size'] + 1) // params['batch_size']\n",
        "predict(model, test_path, test_data, test_data_iterator, num_steps, \"test2.out\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicting NER and saving it to /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/pred_ner\n",
            "reading /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/data/test\n",
            "reading /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/pred_ner\n",
            "appending predicted NER to test2.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbMEjxejFYtW"
      },
      "source": [
        "## Load Model and Predict Dev and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8JEFihGGAHJ"
      },
      "source": [
        "## Predict Test Data and output test2.out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsBSR2hnFYeq",
        "outputId": "e11679cc-0924-4b6a-886a-b0dcaf1b799a"
      },
      "source": [
        "model2_path = base_path+\"blstm2.pt\"\n",
        "\n",
        "load_model_params = {\n",
        " 'batch_size': 5,\n",
        " 'cuda': True,\n",
        " 'embedding_dim': 100,\n",
        " 'linear_output_dim': 128,\n",
        " 'lstm_hidden_dim': 256,\n",
        " 'number_of_tags': 9,\n",
        " 'save_summary_steps': 100,\n",
        " 'test_size': 3683,\n",
        " 'vocab_size': 30292}\n",
        "\n",
        "load_model = Net(load_model_params).cuda()\n",
        "load_model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "val_data_iterator = data_iterator(dev_data, load_model_params, vocab, shuffle=False)\n",
        "num_steps = (dev_data['size'] + 1) // load_model_params['batch_size']\n",
        "predict(model, dev_path, dev_data, val_data_iterator, num_steps, \"dev2_load.out\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.33 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicting NER and saving it to /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/pred_ner\n",
            "reading /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/data/dev\n",
            "reading /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/pred_ner\n",
            "appending predicted NER to dev2_load.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cAwXoE8F32y",
        "outputId": "9ce801ea-e591-4ecb-8f5b-253ee1186393"
      },
      "source": [
        "test_data_iterator = test_iterator(test_data, load_model_params, vocab)\n",
        "load_model = Net(load_model_params).cuda()\n",
        "load_model.load_state_dict(torch.load(model_path))\n",
        "num_steps = (load_model_params['test_size'] + 1) // load_model_params['batch_size']\n",
        "predict(model, test_path, test_data, test_data_iterator, num_steps, \"test2_load.out\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicting NER and saving it to /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/pred_ner\n",
            "reading /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/data/dev\n",
            "reading /content/drive/MyDrive/USC master/CSCI 577 Applied Natural Language Processing/HW4/pred_ner\n",
            "appending predicted NER to dev2_load.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlc5Jt5YGlcv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}